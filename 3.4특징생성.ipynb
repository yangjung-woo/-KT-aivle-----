{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특징 생성: 원시 데이터로 부터 분석에 용이하도록 특징을 생성\n",
    "## [3가지 방안]\n",
    "**1.범주 인코딩**\n",
    "\n",
    " - 숫자가 아닌 범주 변수값을 숫자로 표현하고 모댈링에 적용하기 위한 과정\n",
    "\n",
    "**2.결합 및 분해**\n",
    "    \n",
    " - 데이터셋의 변수들의 조합을 기반으로 새로운 특징을 구축\n",
    " - 변수간의 연산 혹은 분해를 통해 새로운 특징을 구추가고 입력변수로 모델링에 적용\n",
    "    \n",
    "**3.차원 축소**\n",
    "    \n",
    " - 원본 데이터로부터 새로운 특징의 집합을 생성\n",
    " - 고차원 원시 데이터 셋을 저차원으로 차원축소 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 범주 인코딩 \n",
    " - 범주형 데이터의 알고리즘 적용을 위한 수치형 변환\n",
    " 1. One-hot Encoding : K개 범주를 지는 범주형 변수를 k개의 변수로 변환 \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city라는 범주형 번수 one-hot encoding\n",
    "# Pandas 의 get_dummies 함수를 활용 \n",
    "\n",
    "encoding_data = data.copy()\n",
    "encoding_data = pd.get_dummies(encoding_data,columns=['city'])\n",
    "\n",
    "# 기존 city변수 내 5개 범주가 존재\n",
    "# get_dummies 함수를 통해 원본 데이터의 city 변수 대신 각 범주별 변수가 생성 (1개 변수 --> 5개 변수)\n",
    "# city: 서울 >> city_seoul: True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one-hot encoding은 각 범주의 요소마다 별도 컬럼으로 생성하여 True/False를 표현\n",
    "- 기계 학습의 많은 알고리즘은 수치형 데이터를 입력값으로 받아야함\n",
    "- 따라서, 범주형 변수의 One-hot Encoding 기법을 활용하여 기계학습 적용의 제약점을 해소 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결합 기반 특징 생성\n",
    " - 변수간의 결합을 통해 새로운 의미를 지닌 특징을 생성\n",
    " 1. Add(합계) /Divide(평균) /Substract(편차)\n",
    " 2. Multiply(상호작용 항, ex) 온도*습도 상호작용 항으로 온도와 입력값의 시너지 효과 파악\n",
    "     - *단 도메인 지식 기반으로 Multiply를 시행해야함 *\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 분해 기반 특징 생성 \n",
    " - 변수의 분해를 통해 새로운 의미를 지닌 특징 생성\n",
    " 1. separate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간대별 파악 목적\n",
    "# 어느 시간대 혹은 요일 별로 구매 결정이 높아지는지 확인하고자 할 때 \n",
    "\n",
    "creation_data = data.copy()\n",
    "creation_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 범주형 변수인 data 컬럼을 datetime 형식으로 변환 \n",
    "creation_data['date']= pd.to_datetime(creation_data['date'])\n",
    "creation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date 컬럼을 연/월/일/요일 등의 의미를 지는 변수로 분해 \n",
    "creation_data['year'] = creation_data['date'].dt.year\n",
    "creation_data['motth'] = creation_data['date'].dt.month\n",
    "creation_data['day'] = creation_data['date'].dt.day\n",
    "creation_data['hour'] = creation_data['date'].dt.hour\n",
    "creation_data['dayofweek'] = creation_data['date'].dt.dayofweek # 요일 월요일=0 ~ 일요일=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오전/오후의 의미를 지는 변수 생성을 위한 결합 방안 \n",
    "# AM PM 구분 변수 생성 \n",
    "creation_data['ampm'] = 'AM'\n",
    "creation_data.loc[creation_data['hour']>12,'ampm'] = 'PM'\n",
    "creation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 연/월/일 등을 분석가 기준에 맞추어 시간대 의미를 지는 신규 파생 변수로 분해 및 결합 가능\n",
    "- dayofweek 변수의 경우 평일/주말로 구분하는 신규 파생 변수로 분해 및 결합 가능 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 차원 축소 목적 특징 생성\n",
    " - 변수들이 지닌 정보를 최대한 확보하는 저차원 데이터로 생성\n",
    "  1. PCA(주성분 분석): 독립변수간 상관관계가 없도록 구성\n",
    "  2. Featurization via Clustering: 고차원 데이터를 군집분석을 기반으로 차원축소\n",
    "     - 위 결과를 분류/회귀 등 문제해결을 위한 입력변수로 활용(Stacking 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ucimlrepo.dotdict.dotdict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 셋 가져오기\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(breast_cancer_wisconsin_diagnostic.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(breast_cancer_wisconsin_diagnostic.variables) \n",
    "\n",
    "type(breast_cancer_wisconsin_diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0  1      2      3       4       5        6        7        8   \\\n",
      "0      842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010   \n",
      "1      842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690   \n",
      "2    84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740   \n",
      "3    84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140   \n",
      "4    84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800   \n",
      "..        ... ..    ...    ...     ...     ...      ...      ...      ...   \n",
      "564    926424  M  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390   \n",
      "565    926682  M  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400   \n",
      "566    926954  M  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251   \n",
      "567    927241  M  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140   \n",
      "568     92751  B   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000   \n",
      "\n",
      "          9   ...      22     23      24      25       26       27      28  \\\n",
      "0    0.14710  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
      "1    0.07017  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
      "2    0.12790  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
      "3    0.10520  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
      "4    0.10430  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
      "..       ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
      "564  0.13890  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
      "565  0.09791  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
      "566  0.05302  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
      "567  0.15200  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
      "568  0.00000  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
      "\n",
      "         29      30       31  \n",
      "0    0.2654  0.4601  0.11890  \n",
      "1    0.1860  0.2750  0.08902  \n",
      "2    0.2430  0.3613  0.08758  \n",
      "3    0.2575  0.6638  0.17300  \n",
      "4    0.1625  0.2364  0.07678  \n",
      "..      ...     ...      ...  \n",
      "564  0.2216  0.2060  0.07115  \n",
      "565  0.1628  0.2572  0.06637  \n",
      "566  0.1418  0.2218  0.07820  \n",
      "567  0.2650  0.4087  0.12400  \n",
      "568  0.0000  0.2871  0.07039  \n",
      "\n",
      "[569 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "cancer = pd.read_csv('D:\\KTdata\\cancer.data', header=None)\n",
    "print(cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주성분분석 이전에 각 변수간 스케일을 꼭 확인\n",
    "# 스케일이 다를 경우 표준화 수행 후 주성분 분석 수행\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# input_df를 scaling 실행 후 변환 \n",
    "std_scaler.fit(input_df)\n",
    "input_scaled = std_scaler.transform(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주성분 분석\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 두 개 주성분만 유지시키도록 수행 \n",
    "# 30개의 변수 데이터를 2개 주성분만 남도록 변환\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(input_scaled)\n",
    "X_pca = pca.transform(input_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30개의 변수를 지닌 데이터가 2개의 특징들로 압축되었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2개의 주성분으로 구성된 컬럼들이 Target을 구분하기에 효율적이지 시각화로 확인\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 산점도로 2개의 주성분을 시각화 \n",
    "ax = sns.scatterplot(x='pc1' ,y = 'pc2' ,data = X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Targrt과 확인을 위해 주성분 분석을 수행한 input data와 기존 target data 를 merge\n",
    "\n",
    "target_df = target_df.reset_index()\n",
    "pca_df = pd.merge(X_pca_df, target_df, left_index=True , right_index=True,how='inner')\n",
    "pca_df = pca_df.df['pc1','pc2','diagonosis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스를 색깔별로 구분하여 처움 두 개의 주성분으로 Target과 비교\n",
    "ax = sns.scatterplot(x='pc1' ,y = 'pc2',hue='diagonsis' ,data = pca_df,palette=['green','red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실제 모델링에 적용하기 위해 효율적으로 활용 가능한 주성분 분석\n",
    "- 특히, record 및 컬럼이 많은 경우 모델링 연산 비용이 많이 들게 되므로 효율적인 차원 축소 기반의 특징을 생성하는 것이 분석 과정 내 필요한 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주성분을 선택하는 다른 방안\n",
    "- 유지시킬 주성분 개수가 아닌 분산의 설명가능 수준을 비율로 선택 가능\n",
    "    - pca = PCA(n_components = 0.8);\n",
    "    - 주성분의 수는 전체 분산의 최소 80% 수준에서 설명하는 수준에서 자동으로 선택\n",
    "- 이를 통해 수치를 변경하면서 추출되는 주성분의 수 확인 가능하며 이는 분산에 기초한 주성분 개수를 선택하는 부분에서 얼마나 많은 주성분을 사용할 것인지 확인해야 할 때 사용 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 분산의 최소 80% 수준에서 설명하는 수준의 주성분 확보\n",
    "pca = PCA(n_components = 0.8)\n",
    "pca.fit(input_scaled)\n",
    "X_pca = pca.trasnform(input_scaled)\n",
    "X_pca_df= pd.DataFrame(X_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**차원축소 기반 특징 생성(2) Clustering(군집분석)**\n",
    "\n",
    "    - 여러개의 변수를 하나의 변수(군집분석)로 변환 차원축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#일부 변수만 선택\n",
    "# 즉 활용할 정보의 양을 절반으로 축소 \n",
    "subset_df = input_df.loc[:,0:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 스케일링 \n",
    "std_scaler.fit(subset_df)\n",
    "subset_input_scaled= std_scaler.transform(subset_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means 클러스터 활용 \n",
    "k = 5\n",
    "model = KMeans(n_clusters = k , random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling 한 데이터를 fit 하여 모델 학습\n",
    "model.fit(subset_input_scaled)\n",
    "\n",
    "# 클러스터링 결과를 타겟 변수와 비교하기 위하여 원 데이터에 컬럼으로 생성 \n",
    "target_df['cluster'] = model.fit_predict(subset_input_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15개 임의의 변수로 만들어진 하나의 특징(군집분석)과 기존 Target 변수 비교\n",
    "pd.crosstab(target_df.diagonosis, target_df.cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임의의 15개 변수만을 활용한 하나의 특징(군집결과)이 Target 구분에 효과적일 것을 예측 가능함\n",
    "- 이처럼 많은 변수를 하나의 특징으로 구성하고, 입력 데이터의 차원을 줄인다면 모델 연산 비용 절감에 효과적"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
